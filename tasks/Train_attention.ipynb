{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394bbeec",
   "metadata": {},
   "source": [
    "# 45‚Äëminute hands‚Äëon: Fine‚Äëtune a Transformer with ü§ó Trainer + peek at Attention\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/uitml/GenerativeAI_course/blob/main/tasks/Train_attention.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "**Goal (what you'll do):**\n",
    "1. Load a dataset (GLUE MRPC)  \n",
    "2. Tokenize it, build a data collator  \n",
    "3. Fine‚Äëtune a pretrained Transformer using the **Trainer API**  \n",
    "4. Evaluate accuracy/F1  \n",
    "5. **Optional:** visualize one attention head for a single example\n",
    "\n",
    "**Assumptions:** You already had a lecture introducing Transformers/attention.  \n",
    "**Run on:** Colab with GPU (T4/L4/A100).  \n",
    "\n",
    "> Tip: If something fails, restart runtime and rerun from the top.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7b01c3",
   "metadata": {},
   "source": [
    "## Instructor notes (suggested timing)\n",
    "\n",
    "- **0‚Äì5 min:** Setup + GPU check  \n",
    "- **5‚Äì12 min:** Load dataset + create subset (discuss why subsets + epochs matter)  \n",
    "- **12‚Äì20 min:** Tokenization + padding/collator  \n",
    "- **20‚Äì35 min:** Trainer fine-tune (students watch loss/metrics)  \n",
    "- **35‚Äì40 min:** Quick predictions + interpretation  \n",
    "- **40‚Äì45 min:** Optional attention heatmap (connect back to lecture)\n",
    "\n",
    "**Common pitfalls**\n",
    "- Forgot GPU ‚Üí training is slow  \n",
    "- Library version conflicts ‚Üí restart runtime, rerun install cell  \n",
    "- If `evaluate.load` downloads slowly, wait once; it caches afterward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc65fe",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "### (A) Turn on GPU\n",
    "In Colab: **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**\n",
    "\n",
    "### (B) Install libraries\n",
    "We pin a *reasonable* minimum set and keep it light.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (quiet-ish). If you see version conflicts, restart runtime and rerun this cell.\n",
    "!pip -q install -U \"transformers>=4.41\" \"datasets>=2.20\" \"evaluate>=0.4.2\" \"accelerate>=0.33\" \"torch>=2.1\" \"numpy\" \"matplotlib\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, numpy as np, torch\n",
    "from packaging import version\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "import transformers, datasets, evaluate, accelerate, matplotlib\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"evaluate:\", evaluate.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "\n",
    "# Sanity check: GPU available?\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "if device != \"cuda\":\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be slow. Turn on GPU in Colab and restart runtime.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991a51c",
   "metadata": {},
   "source": [
    "## 1) Load a dataset (GLUE MRPC)\n",
    "\n",
    "MRPC: sentence-pair paraphrase classification. Small enough for a short class, but ‚Äúreal‚Äù.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw = load_dataset(\"glue\", \"mrpc\")\n",
    "raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfb1f2",
   "metadata": {},
   "source": [
    "### Keep it fast: use a small subset for training\n",
    "\n",
    "For a 45‚Äëminute class, we'll train on a subset so **everyone finishes**.\n",
    "You can increase `train_size` if you have stronger GPUs or more time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 2000   # try 5000+ if you have time\n",
    "eval_size  = 500    # small eval for speed\n",
    "\n",
    "train_ds = raw[\"train\"].shuffle(seed=42).select(range(min(train_size, len(raw[\"train\"]))))\n",
    "eval_ds  = raw[\"validation\"].shuffle(seed=42).select(range(min(eval_size, len(raw[\"validation\"]))))\n",
    "\n",
    "len(train_ds), len(eval_ds), train_ds[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250ea90",
   "metadata": {},
   "source": [
    "## 2) Tokenize + data collator\n",
    "\n",
    "We‚Äôll use a compact model for speed. `distilbert-base-uncased` is a good default.\n",
    "(You can swap to `bert-base-uncased` if you want a heavier baseline.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e099d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"   # fast\n",
    "# checkpoint = \"bert-base-uncased\"      # slower, but classic\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"sentence1\"], batch[\"sentence2\"], truncation=True)\n",
    "\n",
    "tok_train = train_ds.map(tokenize_batch, batched=True, remove_columns=train_ds.column_names)\n",
    "tok_eval  = eval_ds.map(tokenize_batch, batched=True, remove_columns=eval_ds.column_names)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tok_train, tok_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727e05a",
   "metadata": {},
   "source": [
    "## 3) Define model + metrics\n",
    "\n",
    "We‚Äôll use a sequence classification head (2 labels).\n",
    "Metrics: **accuracy** and **F1** (standard for MRPC).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a150e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Load metric once (faster + avoids repeated downloads inside compute_metrics)\n",
    "mrpc_metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return mrpc_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f68c9",
   "metadata": {},
   "source": [
    "## 4) Fine‚Äëtune with the Trainer API\n",
    "\n",
    "This cell runs the full training loop: batching, forward, loss, backward, optimizer, evaluation.\n",
    "\n",
    "We set conservative hyperparameters so it finishes quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fc709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "out_dir = \"mrpc-distilbert-trainer\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),  # mixed precision on GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_train,\n",
    "    eval_dataset=tok_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f9d8e",
   "metadata": {},
   "source": [
    "## 5) Evaluate and try the model\n",
    "\n",
    "We‚Äôll compute metrics on the eval subset and run a few predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = trainer.evaluate()\n",
    "eval_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91abd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_pair(s1, s2):\n",
    "    inputs = tokenizer(s1, s2, return_tensors=\"pt\", truncation=True).to(trainer.model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = trainer.model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy().round(4)[0]\n",
    "    label = int(np.argmax(probs))\n",
    "    return {\"pred_label\": label, \"p(not paraphrase)\": float(probs[0]), \"p(paraphrase)\": float(probs[1])}\n",
    "\n",
    "examples = [\n",
    "    (\"The company said it will cut costs.\", \"The firm announced cost reductions.\"),\n",
    "    (\"He likes pizza.\", \"The capital of France is Paris.\"),\n",
    "]\n",
    "for s1, s2 in examples:\n",
    "    print(s1)\n",
    "    print(s2)\n",
    "    print(predict_pair(s1, s2))\n",
    "    print(\"‚Äî\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14b922",
   "metadata": {},
   "source": [
    "## 6) Optional: Peek at attention (single example)\n",
    "\n",
    "This is **not** how you ‚Äúexplain attention‚Äù (you already did that in lecture), but it helps students connect the idea\n",
    "to something tangible: attention matrices per layer/head.\n",
    "\n",
    "We‚Äôll:\n",
    "1. Run a forward pass with `output_attentions=True`\n",
    "2. Pick a layer/head\n",
    "3. Plot attention weights\n",
    "\n",
    "> Note: Attention matrices are over **token positions** (including special tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f20cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pick one example from eval set\n",
    "ex = eval_ds[0]\n",
    "s1, s2 = ex[\"sentence1\"], ex[\"sentence2\"]\n",
    "print(\"Sentence1:\", s1)\n",
    "print(\"Sentence2:\", s2)\n",
    "\n",
    "inputs = tokenizer(s1, s2, return_tensors=\"pt\", truncation=True)\n",
    "inputs = {k: v.to(trainer.model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Force attentions\n",
    "with torch.no_grad():\n",
    "    outputs = trainer.model(**inputs, output_attentions=True)\n",
    "attentions = outputs.attentions  # tuple: (num_layers, batch, num_heads, seq, seq)\n",
    "\n",
    "len(attentions), attentions[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose layer/head\n",
    "layer_idx = 0\n",
    "head_idx = 0\n",
    "\n",
    "att = attentions[layer_idx][0, head_idx].detach().cpu().numpy()  # (seq, seq)\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0].detach().cpu().tolist())\n",
    "\n",
    "print(\"num_tokens:\", len(tokens))\n",
    "print(tokens[:20], \"...\" if len(tokens) > 20 else \"\")\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.imshow(att)\n",
    "plt.title(f\"Attention heatmap ‚Äî layer {layer_idx}, head {head_idx}\")\n",
    "plt.xlabel(\"Key position\")\n",
    "plt.ylabel(\"Query position\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f88552",
   "metadata": {},
   "source": [
    "## 7) Optional extension (if time): make it ‚Äúmore LLM‚Äù\n",
    "\n",
    "If you have extra time, you can adapt this notebook to **causal language modeling** (next-token prediction) and train with\n",
    "TRL‚Äôs SFTTrainer + LoRA (parameter‚Äëefficient fine‚Äëtuning).  \n",
    "This is a bigger jump in compute + dependencies, so we keep it as an after‚Äëclass exercise.\n",
    "\n",
    "- TRL notebooks: https://huggingface.co/docs/trl/en/example_overview  \n",
    "- ‚ÄúSFT with LoRA/QLoRA‚Äù Colab: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
